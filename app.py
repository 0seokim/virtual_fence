import streamlit as st
import cv2 , math
import numpy as np
from ultralytics import YOLO
from ensemble_boxes import weighted_boxes_fusion
from utils import *
# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide")
st.title("YOLO Virtual Fence Detection")


model_mode = st.sidebar.selectbox(
    "Model Mode", ["Base Model", "Finetuned Models"]
)

# Single ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° 
@st.cache_resource
def load_basemodel():
    model = YOLO("yolov8n.pt")  # í•„ìš”í•œ ê²½ìš° custom ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½
    return model

# Ensemble ëª¨ë¸ë“¤ ë¡œë“œ
@st.cache_resource
def load_ensemble():
    return [
        YOLO(r"/mnt/c/Users/kimyo/OneDrive/ë°”íƒ• í™”ë©´/streamlit/model5_best.pt"),  # pretrained on data1
        YOLO(r"/mnt/c/Users/kimyo/OneDrive/ë°”íƒ• í™”ë©´/streamlit/model6_best.pt"),  # pretrained on data2
        YOLO(r"/mnt/c/Users/kimyo/OneDrive/ë°”íƒ• í™”ë©´/streamlit/model4_best.pt"),  # pretrained on data3
    ]

if model_mode == "Base Model":
    model = load_basemodel()            # í•œ ê°œ
else:
    models = load_ensemble()


# ì…ë ¥ ì„ íƒ
input_mode = st.sidebar.selectbox("Select Input Mode", ["Webcam", "Video file from path"])
# íœìŠ¤ ëª¨ì–‘ ì„ íƒ
shape = st.sidebar.selectbox("Fence Shape", ["Rectangle", "Triangle", "Circle"])  # ğŸ”§

video_path = None  # ê¸°ë³¸ê°’

# ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
if input_mode == "Video file from path":
    video_path = st.sidebar.text_input(
        "Enter full path to video file",
        value="/mnt/c/Users/kimyo/OneDrive/ë°”íƒ• í™”ë©´/streamlit/3562070-hd_1920_1080_30fps.mp4",  # ì˜ˆì‹œ ê²½ë¡œ
        # help="ì˜ˆ: /home/user/videos/sample.mp4 ë˜ëŠ” C:\\Users\\user\\Videos\\test.mp4"
    )

# ìŠ¤íŠ¸ë¦¼ë¦¿ ì´ë¯¸ì§€ ì¶œë ¥ì„ ìœ„í•œ ê³µê°„
frame_display = st.empty()

# í•´ìƒë„ ê³„ì‚°
cap0 = get_video_capture(input_mode, video_path)
if not cap0:
    st.error("âš ï¸ ì˜ìƒ ì†ŒìŠ¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
ret, frame0 = cap0.read()
cap0.release()
if not ret:
    st.error("âš ï¸ ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

h, w = frame0.shape[:2]
diag = math.hypot(w, h)
square = int(diag * 0.3)
cx, cy = w//2, h//2
# ìŠ¬ë¼ì´ë” ë²”ìœ„: 0 ~ w or h
st.sidebar.header("Virtual Fence Settings")
x1 = st.sidebar.slider("Fence X1", 0, w, max(0, cx-square//2), key="f_x1")
y1 = st.sidebar.slider("Fence Y1", 0, h, max(0, cy-square//2), key="f_y1")
x2 = st.sidebar.slider("Fence X2", 0, w, min(w, cx+square//2), key="f_x2")
y2 = st.sidebar.slider("Fence Y2", 0, h, min(h, cy+square//2), key="f_y2")

# ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
def run_detection():
    cap = get_video_capture(input_mode, video_path)
    if not cap or not cap.isOpened():
        st.warning("ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    f_x1 = st.session_state.f_x1
    f_y1 = st.session_state.f_y1
    f_x2 = st.session_state.f_x2
    f_y2 = st.session_state.f_y2


    fence_rect = (f_x1, f_y1, f_x2, f_y2)

    # first = True  # ì²« í”„ë ˆì„ ì—¬ë¶€
    while cap.isOpened():
        success, frame = cap.read()
        if not success:break
        
         # ğŸ”§ ëª¨ë¸ ëª¨ë“œì— ë”°ë¼ predict ë¶„ê¸°
        if model_mode == "Base Model":
            results = model.predict(source=frame, classes=[0], verbose=False)[0]
            boxes = [tuple(map(int, b.xyxy[0])) for b in results.boxes]
            scores = [float(b.conf[0]) for b in results.boxes]
            classes = [int(b.cls[0]) for b in results.boxes]
        else:
            # ensemble ë¡œì§ (models ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
            all_b, all_s, all_c = [], [], []
            h, w = frame.shape[:2]
            for m in models:
                results = m.predict(source=frame, classes=[0], verbose=False)[0]
                bn, sc, cl = [], [], []
                for b in results.boxes:
                    x1,y1,x2,y2 = b.xyxy[0]
                    bn.append([x1/w, y1/h, x2/w, y2/h])
                    sc.append(float(b.conf[0]))
                    cl.append(int(b.cls[0]))
                all_b.append(bn); all_s.append(sc); all_c.append(cl)
            if any(all_b):
                bn, scores, classes = weighted_boxes_fusion(all_b, all_s, all_c,
                                                           iou_thr=0.5,
                                                           skip_box_thr=0.25)
                boxes = [(int(x1*w), int(y1*h), int(x2*w), int(y2*h)) for x1,y1,x2,y2 in bn]
            else:
                boxes, scores, classes = [], [], []
        
        annotated_frame = frame.copy()
        person_inside = False


        for (x_min, y_min, x_max, y_max), score, cls in zip(boxes, scores, classes):
        # í´ë˜ìŠ¤ê°€ 0(person)ì¸ì§€ ì¬í™•ì¸
            if cls == 0:
                inside = is_overlapping_fence((x_min, y_min, x_max, y_max), fence_rect)
                if inside:
                    person_inside = True

                color = (0, 0, 255) if inside else (255, 255, 0)
                label = f"Human {score:.2f}"
                cv2.rectangle(annotated_frame, (int(x_min), int(y_min)),
                                (int(x_max), int(y_max)), color, 2)
                cv2.putText(annotated_frame, label, (int(x_min), int(y_min)-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        if results.boxes is not None:
            for box in results.boxes:
                cls = int(box.cls[0])
                if cls == 0:  # person
                    x_min, y_min, x_max, y_max = map(int, box.xyxy[0])
                    box_coords = (x_min, y_min, x_max, y_max)
                    # fence_rect = (x1, y1, x2, y2)

                    inside = is_overlapping_fence(box_coords, fence_rect)
                    if inside:
                        person_inside = True

                    color = (0, 0, 255) if inside else (255, 255, 0)  # ë¹¨ê°„ìƒ‰ or ë…¸ë€ìƒ‰
                    label = "Human (in fence)" if inside else "Human"
                    cv2.rectangle(annotated_frame, (x_min, y_min), (x_max, y_max), color, 2)
                    cv2.putText(annotated_frame, label, (x_min, y_min - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        current_fence_color = (0, 0, 255) if person_inside else (0, 255, 0)

        # Fence ë‚´ë¶€ ì±„ì›€ (ì—°í•˜ê²Œ)
        overlay = annotated_frame.copy()
        cv2.rectangle(overlay, (f_x1, f_y1), (f_x2, f_y2), current_fence_color, -1)
        alpha = 0.2
        annotated_frame = cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0)


        # ìŠ¤íŠ¸ë¦¼ë¦¿ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
        cv2.rectangle(annotated_frame, (f_x1, f_y1), (f_x2, f_y2), current_fence_color, 2)
        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        frame_display.image(annotated_frame, channels="RGB")

    cap.release()

# ì‹¤í–‰ ë²„íŠ¼
if st.button("â–¶ï¸ Start Detection"):
    run_detection()
